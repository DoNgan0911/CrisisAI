# -*- coding: utf-8 -*-
"""SA_Locations_refining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CyUrVjqN5DYFcjGjrlR3pRhp-Me40cEX
"""

import pandas as pd, json

# Load CrisisNLP (HumAID) file
#crisis = pd.read_csv("california_wildfires_2018_train.tsv", sep="\t")

# Load IDRISI file
with open("train_idris.jsonl") as f:
    idrisi = [json.loads(line) for line in f]
len(idrisi)

import pandas as pd, glob, json

# Merge train + dev + test from CrisisNLP
files = glob.glob("california_wildfires_2018_*.tsv")
dfs = [pd.read_csv(p, sep="\t", dtype={"tweet_id":str}) for p in files]
crisis_all = pd.concat(dfs, ignore_index=True).drop_duplicates("tweet_id")
print("CrisisNLP CA-2018 total tweets:", crisis_all["tweet_id"].nunique())

# Load IDRISI
idrisi = [json.loads(line) for line in open("train_idris.jsonl")]
idrisi_df = pd.DataFrame(idrisi)
idrisi_df["tweet_id"] = idrisi_df["tweet_id"].astype(str)

# Compare overlap
common = set(idrisi_df["tweet_id"]) & set(crisis_all["tweet_id"])
print("Overlap:", len(common), "out of", len(idrisi_df))

import matplotlib.pyplot as plt
idrisi_df['humAID_class'].value_counts()

idrisi_df.head(6)

import ast

# If location_mentions is stored as string, first convert to Python objects
idrisi_df["location_mentions"] = idrisi_df["location_mentions"].apply(
    lambda x: ast.literal_eval(str(x)) if isinstance(x, str) else x
)

# Extract only the "text" field from each location dictionary
idrisi_df["location_names"] = idrisi_df["location_mentions"].apply(
    lambda locs: [loc["text"] for loc in locs] if isinstance(locs, list) else []
)

# Quick check
print(idrisi_df[["text", "location_mentions", "location_names"]].head(6))

pd.set_option("display.max_rows", 1000)
idrisi_df["location_names"].value_counts(dropna=False)

import re, math, random
from datetime import datetime, timedelta

no_location_tweets = idrisi_df[idrisi_df["location_names"].apply(lambda x: len(x) == 0) & (idrisi_df["humAID_class"] != 'requests_or_urgent_needs')]
len(no_location_tweets)

import pandas as pd
no_location_tweets.to_csv("SA_no_location_tweets.csv", index=False)

no_location_tweets.head(6)

CN = idrisi_df[idrisi_df["humAID_class"] == 'requests_or_urgent_needs' ]

CN_with_location_names = CN[CN["location_names"].apply(lambda x: len(x) > 0)]

len(CN_with_location_names)

SA = idrisi_df[idrisi_df["humAID_class"] != 'requests_or_urgent_needs']

SA['location_names'].value_counts()

SA_with_location_names = SA[SA["location_names"].apply(lambda x: len(x) > 0)]

SA_with_location_names['humAID_class'].value_counts()

CN_with_location_names.to_csv("CN_with_location_names.csv", index=False)
SA_with_location_names.to_csv("SA_with_location_names.csv", index=False)

"""# REFINING LOCATIONS"""

# Lat/lon from OpenCage
HOTSPOTS = [
    # Camp Fire (Butte) -NORCAL
    {"name": "Butte", "type": "county", "fire": "Camp", "lat": 39.6519,  "lon": -121.59},
    {"name": "Paradise",     "type": "city",   "fire": "Camp", "lat": 39.7596, "lon": -121.6219},
    {"name": "Chico",        "type": "city",   "fire": "Camp", "lat": 39.7285, "lon": -121.8375},
    {"name": "Magalia",      "type": "city",   "fire": "Camp", "lat": 39.8121, "lon": -121.5783},
    {"name": "Enloe Medical Center (Chico)",       "type": "facility", "fire": "Camp", "lat": 39.7424, "lon": -121.8502},
    {"name": "Silver Dollar Fairgrounds (Chico)",  "type": "facility", "fire": "Camp", "lat": 39.7174, "lon": -121.8114},

    # Woolsey / Hill (LA & Ventura) - SOCAL
    {"name": "Los Angeles", "type": "county", "fire": "Woolsey", "lat": 34.3155, "lon": -118.2097},
    {"name": "Ventura",     "type": "county", "fire": "Woolsey", "lat": 34.4458, "lon": -119.0780},
    {"name": "Malibu",             "type": "city",   "fire": "Woolsey", "lat": 34.0356, "lon": -118.6894},
    {"name": "Agoura Hills",       "type": "city",   "fire": "Woolsey", "lat": 34.1482, "lon": -118.7655},
    {"name": "Thousand Oaks",      "type": "city",   "fire": "Woolsey", "lat": 34.1706, "lon": -118.8376},
    {"name": "Camarillo",          "type": "city",   "fire": "Hill",    "lat": 34.2176, "lon": -119.0384},
    {"name": "Pepperdine University (Malibu)", "type": "facility", "fire": "Woolsey", "lat": 34.0430, "lon": -118.7093},

    # Nurse (Solano) - NORCAL
    {"name": "Solano", "type": "county", "fire": "Nurse",   "lat": 38.2219, "lon": -121.9164},
    {"name": "Vacaville",      "type": "city",   "fire": "Nurse",   "lat": 38.3566, "lon": -121.9877},

]

import re

def canon(s: str) -> str:
    s0 = s.strip().lower()
    s0 = re.sub(r"[–—•·]", "-", s0)   # normalize special dashes/dots
    s0 = re.sub(r"\s+", " ", s0)      # collapse spaces
    for pat, rep in CANON_EQUIV.items():
        if re.match(pat, s0):
            return rep
    return s0

# --- Aliases so common variants hit your hotspot records ---
ALIASES = {
    "los angeles county": "los angeles",
    "ventura county": "ventura",
    "butte county": "butte",
    "solano county": "solano",
    "la": "los angeles",
    "l.a.": "los angeles",
    "agoura": "agoura hills",
    "t.o.": "thousand oaks",
    "pepperdine": "pepperdine university (malibu)",
    "pepperdine univ": "pepperdine university (malibu)",
}

# --- Rebuild canonical maps with the real canon() ---
CANON_NAME2HS = {canon(h["name"]): h for h in HOTSPOTS}

SYNONYM2HS = {}
for k, v in ALIASES.items():
    k_c = canon(k)
    v_c = canon(v)
    if v_c in CANON_NAME2HS:
        SYNONYM2HS[k_c] = CANON_NAME2HS[v_c]

def get_hotspot_by_name(name: str):
    key = canon(name)
    if key in CANON_NAME2HS:
        return CANON_NAME2HS[key]
    if key in SYNONYM2HS:
        return SYNONYM2HS[key]
    return None

ALL_HOTSPOTS = HOTSPOTS[:]
NOCAL_POOL = [h for h in HOTSPOTS if h["fire"] in {"Camp", "Nurse"}]
SOCAL_POOL = [h for h in HOTSPOTS if h["fire"] in {"Woolsey","Hill"}]

#The code standardizes location names and filters them so only California-related ones are kept
# --------------- Canonicalization helpers -------------
CANON_EQUIV = {
    # California variants
    r"^cal(ifornia)?s?$": "california",
    r"^ca$": "california",
    r"^cali$": "california",
    r"^calif\.?$": "california",
    r"^californias?$": "california",
    r"^california--these$": "california",
    r"^️?california$": "california",
    r"^california,$": "california",
    r"^california\)$": "california",
    r"^california, ca, california$": "california",
    r"^ca, ca$": "california",
    r"^caifornia$": "california",
    r"^californian$": "california",
    r"^californians$": "california",

    # Regions
    r"^northern calif(ornia|ornias|\.|o)?$": "northern california",
    r"^southern calif(ornia|ornias)?$": "southern california",
    r"^norcal$": "northern california",
    r"^socal$": "southern california",

    # Known hotspots synonyms
    r"^los angeles county, california$": "los angeles",
    r"^ventura county, california$": "ventura",
    r"^butte county, california$": "butte",
    r"^paradise, ca(lifornia)?$": "paradise",
    r"^chico, ca(lifornia)?$": "chico",
    r"^malibu, california$": "malibu",
    r"^los ?angeles$": "los angeles",
    r"^losangeles$": "los angeles",
    r"^los angeles, ca(lifornia)?$": "los angeles",

    r"^l\.?a\.?$": "los angeles",
    r"^los\s*angeles\s*co\.?(unty)?$": "los angeles",
    r"^ventura\s*co\.?(unty)?$": "ventura",
    r"^butte\s*co\.?(unty)?$": "butte",
    r"^agoura$": "agoura hills",
    r"^t\.?o\.?$": "thousand oaks",
    r"^pepperdine$": "pepperdine university (malibu)",
    r"^pepperdine\s+univ(\.|ersity)?$": "pepperdine university (malibu)",
    r"^paradise,\s*ca$": "paradise",
    r"^chico,\s*ca$": "chico",

}

NON_CA_NOISE = {
    "texas","florida","georgia","oregon","puerto rico","mexico","manhattan","nyc",
    "denver","ohio","missouri","virginia","kashmir","beijing","sacramento","san francisco",
    "japan","canada","saudi arabia","u.s.","united states","usa","the united states",
    "lincoln","dublin","oakland","berkeley","memphis","oklahoma","san diego","stockton",
}

def canon(s: str) -> str:
    s0 = s.strip().lower()
    s0 = re.sub(r"[–—•·]", "-", s0)
    s0 = re.sub(r"[\s]+", " ", s0)
    for pat, rep in CANON_EQUIV.items():
        if re.match(pat, s0):
            return rep
    return s0

#checks if a place is inside California (or one of its fire hotspots), and filters out obvious out-of-state noise
CALIF_PAT = re.compile(r"\bcalifornia\b")
CA_COUNTY_PAT = re.compile(r"\b(los angeles|ventura|butte|solano)\s+(co\.?|county)\b")

EXTRA_CA_NAMES = {
    "los angeles","ventura","butte","paradise","chico","magalia",
    "malibu","agoura hills","thousand oaks","camarillo","vacaville","big pine","solano"
}

def is_ca_scope(s: str) -> bool:
    s = canon(s)
    if s in {"california", "northern california", "southern california"}:
        return True
    if s in NAME2HS:  # keep your legacy fast path
        return True
    if s in NON_CA_NOISE:
        return False
    if CALIF_PAT.search(s):
        return True
    if CA_COUNTY_PAT.search(s):
        return True
    return s in EXTRA_CA_NAMES

import re

# ------------------ Selection rules ------------------
""" SELECTION RULES:
It takes tweet location mentions → cleans them → keeps only California ones →
if a known hotspot is named, pick the most specific (facility > city > county).
Otherwise, guess by region hints (NorCal/SoCal) and randomly pick a hotspot.
For CN tweets, prefer city/facility; for SA tweets, counties are allowed.
"""

CN_LABEL = "requests_or_urgent_needs"

FIRE_NAME_PATTERNS = {
    "Camp":    re.compile(r"\b#?camp\s*fire\b|#?campfire\b|camp\b.*\bblaze\b", re.I),
    "Woolsey": re.compile(r"\b#?woolsey\s*fire\b|#?woolseyfire\b|woolsey\b.*\bblaze\b", re.I),
    "Hill":    re.compile(r"\b#?hill\s*fire\b|#?hillfire\b|hill\b.*\bblaze\b", re.I),
    "Nurse":   re.compile(r"\b#?nurse\s*fire\b|#?nursefire\b|nurse\b.*\bblaze\b", re.I),
}
def detect_fire_name(tweet_text: str) -> str | None:
    if not tweet_text:
        return None
    for fire, pat in FIRE_NAME_PATTERNS.items():
        if pat.search(tweet_text):
            return fire
    return None

def pool_for_fire(fire: str):
    return [h for h in HOTSPOTS if h["fire"] == fire]

    # --- ADD: helpers for reporting ---
def pool_tag_for_fire(fire: str) -> str:
    if fire in {"Camp", "Nurse"}:
        return "norcal"
    if fire in {"Woolsey", "Hill"}:
        return "socal"
    return "mixed"

#Looks for regional hints in the (already-canonicalized) mentions:
# def choose_pool_by_mentions(canon_mentions, fire_hint: str | None = None):
#     if fire_hint:
#         return pool_for_fire(fire_hint)
#     if any(m == "northern california" or "butte" in m or "paradise" in m or "chico" in m for m in canon_mentions):
#         return NOCAL_POOL
#     if any(m == "southern california" or "los angeles" in m or "ventura" in m or "malibu" in m or "agoura" in m or "thousand oaks" in m for m in canon_mentions):
#         return SOCAL_POOL
#     return ALL_HOTSPOTS
# --- REPLACE: choose_pool_by_mentions to return (pool, reason) ---
def choose_pool_by_mentions(canon_mentions, fire_hint: str | None = None):
    """
    Returns:
      pool: list of hotspots to sample from
      reason: one of 'fire_hint:<name>', 'region:norcal', 'region:socal', 'fallback:all_hotspots'
    """
    if fire_hint:
        return pool_for_fire(fire_hint), f"fire_hint:{fire_hint}"
    if any(m == "northern california" or "butte" in m or "paradise" in m or "chico" in m for m in canon_mentions):
        return NOCAL_POOL, "region:norcal"
    if any(m == "southern california" or "los angeles" in m or "ventura" in m or "malibu" in m or "agoura" in m or "thousand oaks" in m for m in canon_mentions):
        return SOCAL_POOL, "region:socal"
    return ALL_HOTSPOTS, "fallback:all_hotspots"


#For CN tweets (requests), prefer city/facility records if available; else keep pool as-is. Then pick a random hotspot.
def pick_from_pool(pool, is_cn: bool):
    if is_cn:
        pool = [h for h in pool if h["type"] in {"city","facility"}] or pool
    return random.choice(pool)

# def resolve_location(location_names_list, humaid_label, tweet_text: str = ""):
#     is_cn = (humaid_label == CN_LABEL)
#     raw = location_names_list or []
#     canon_list = [canon(x) for x in raw]

#     # 1) Filter CA
#     ca_list = [m for m in canon_list if is_ca_scope(m)]

#     # 2) Nếu khớp hotspot cụ thể → giữ nguyên (facility > city > county)
#     specificity = {"facility":3,"city":2,"county":1}
#     hotspot_matches = [NAME2HS[x] for x in ca_list if x in NAME2HS]
#     if hotspot_matches:
#         hotspot_matches.sort(key=lambda h: specificity.get(h["type"],0), reverse=True)
#         chosen = hotspot_matches[0]
#         if is_cn and chosen["type"]=="county":
#             pool = [h for h in HOTSPOTS if h["fire"]==chosen["fire"] and h["type"] in {"city","facility"}]
#             if pool:
#                 chosen = random.choice(pool)
#         return {"place": chosen["name"], "type": chosen["type"], "lat": chosen["lat"], "lon": chosen["lon"], "fire": chosen["fire"]}

#     # 3) Dò tên đám cháy từ text (override routing vùng)
#     fire_hint = detect_fire_name(tweet_text)

#     # 4) Chọn pool (ưu tiên fire_hint nếu có)
#     pool = choose_pool_by_mentions(ca_list or canon_list, fire_hint=fire_hint)
#     chosen = pick_from_pool(pool, is_cn=is_cn)
#     return {"place": chosen["name"], "type": chosen["type"], "lat": chosen["lat"], "lon": chosen["lon"], "fire": chosen["fire"]}

def resolve_location(location_names_list, humaid_label, tweet_text: str = ""):
    is_cn = (humaid_label == CN_LABEL)
    raw = location_names_list or []
    canon_list = [canon(x) for x in raw]

    # 1) Filter CA
    ca_list = [m for m in canon_list if is_ca_scope(m)]

    # 2) Exact hotspot match → no refinement needed
    specificity = {"facility":3,"city":2,"county":1}
    hotspot_matches = []
    for x in ca_list:
      h = get_hotspot_by_name(x)
      if h:
        hotspot_matches.append(h)
    if hotspot_matches:
        hotspot_matches.sort(key=lambda h: specificity.get(h["type"],0), reverse=True)
        chosen = hotspot_matches[0]
        decision = "exact_hotspot"
        decision_detail = chosen["name"].lower()

        # For CN, prefer city/facility within same fire if possible
        if is_cn and chosen["type"]=="county":
            pool = [h for h in HOTSPOTS if h["fire"]==chosen["fire"] and h["type"] in {"city","facility"}]
            if pool:
                chosen = random.choice(pool)
                decision = "exact_hotspot→refined_within_fire_for_cn"

        return {
            "place": chosen["name"], "type": chosen["type"], "lat": chosen["lat"], "lon": chosen["lon"],
            "fire": chosen["fire"], "decision": decision, "decision_detail": decision_detail,
            "pool_group": pool_tag_for_fire(chosen["fire"])
        }

    # 3) Fire name from text (override regional routing)
    fire_hint = detect_fire_name(tweet_text)

    # 4) Choose pool (prefer fire_hint if any)
    pool, reason = choose_pool_by_mentions(ca_list or canon_list, fire_hint=fire_hint)
    chosen = pick_from_pool(pool, is_cn=is_cn)

    # decision tags for analytics
    if reason.startswith("fire_hint:"):
        decision = "pool_by_fire_hint"
        decision_detail = reason.split(":",1)[1]
    elif reason == "region:norcal":
        decision = "pool_by_region"
        decision_detail = "norcal"
    elif reason == "region:socal":
        decision = "pool_by_region"
        decision_detail = "socal"
    else:
        decision = "fallback_all_hotspots"
        decision_detail = "none"

    return {
        "place": chosen["name"], "type": chosen["type"], "lat": chosen["lat"], "lon": chosen["lon"],
        "fire": chosen["fire"], "decision": decision, "decision_detail": decision_detail,
        "pool_group": pool_tag_for_fire(chosen["fire"])
    }

# ------------------ Geo helpers (jitter) ------------------
#Adds a tiny random shift (within 2km) around the given point->so multiple tweets at the same hotspot don’t all have exactly identical coordinates (looks more realistic on a map)
def jitter_within_radius(lat, lon, radius_km=2.0):
    if lat is None or lon is None or radius_km <= 0:
        return lat, lon
    R = 6371.0
    bearing = random.uniform(0, 2*math.pi)
    dist = random.uniform(0, radius_km)
    lat1, lon1 = math.radians(lat), math.radians(lon)
    ang = dist / R
    lat2 = math.asin(math.sin(lat1)*math.cos(ang) + math.cos(lat1)*math.sin(ang)*math.cos(bearing))
    lon2 = lon1 + math.atan2(math.sin(bearing)*math.sin(ang)*math.cos(lat1),
                             math.cos(ang) - math.sin(lat1)*math.sin(lat2))
    lon2 = ((math.degrees(lon2)+540) % 360) - 180
    return math.degrees(lat2), lon2
#pick a hotspot and add small random coordinate shift → tag with CN/SA info → return a clean dict.
def refine_row(loc_list, humaid_label, jitter_km=2.0, tweet_text=""):
    resolved = resolve_location(loc_list, humaid_label, tweet_text=tweet_text)
    lat_j, lon_j = jitter_within_radius(resolved["lat"], resolved["lon"], jitter_km)
    resolved["lat"], resolved["lon"] = lat_j, lon_j
    resolved["is_cn"] = (humaid_label == CN_LABEL)
    resolved["label"] = humaid_label
    return resolved

import re, math, random, ast
import pandas as pd

# --- load ---
df = pd.read_csv("SA_with_location_names.csv")

# --- robust parser for the 'location_names' column ---
def parse_location_names(cell):
    """
    Return a clean list[str] of location names.
    Handles:
      - NaN/None
      - Proper Python lists: ["California","Los Angeles"]
      - List of dicts (if it happens): [{"text":"California"}, ...]
      - Sloppy strings: [California, Los Angeles]
      - Plain strings: "California"
    """
    if pd.isna(cell):
        return []
    if isinstance(cell, list):
        # Could be list[str] or list[dict]
        out = []
        for x in cell:
            if isinstance(x, dict) and "text" in x:
                out.append(str(x["text"]))
            else:
                out.append(str(x))
        return out

    s = str(cell).strip()

    # Try literal_eval (works if it's like "['California','Los Angeles']" or [{"text":...}, ...])
    try:
        val = ast.literal_eval(s)
        if isinstance(val, list):
            out = []
            for x in val:
                if isinstance(x, dict) and "text" in x:
                    out.append(str(x["text"]))
                else:
                    out.append(str(x))
            return out
        # If it's a single string, wrap it
        if isinstance(val, str):
            return [val]
    except Exception:
        pass

    # Handle sloppy "[California, Los Angeles]" (no quotes)
    if s.startswith("[") and s.endswith("]"):
        inner = s[1:-1].strip()
        if inner == "":
            return []
        # split on commas that separate items
        items = [x.strip() for x in inner.split(",")]
        # remove stray quotes/whitespace
        items = [x.strip(" '\"") for x in items if x.strip(" '\"")]
        return items

    # Fallback: treat as a single name
    return [s]

# --- apply your resolver to each row ---
# def apply_refine(row):
#     loc_list = parse_location_names(row.get("location_names", None))
#     label = row.get("humAID_class", "")
#     text  = row.get("text", "")  # CSV cần có cột 'text'
#     out = refine_row(loc_list, label, jitter_km=2.0, tweet_text=text)
#     return pd.Series({
#         "place": out["place"],
#         "place_type": out["type"],
#         "lat": out["lat"],
#         "lon": out["lon"],
#         "fire": out["fire"],
#         "is_cn": out["is_cn"],
#         "label": out["label"],
#     })
def apply_refine(row):
    loc_list = parse_location_names(row.get("location_names", None))
    label = row.get("humAID_class", "")
    text  = row.get("text", "")  # CSV needs 'text' column
    out = refine_row(loc_list, label, jitter_km=2.0, tweet_text=text)
    return pd.Series({
        "place": out["place"],
        "place_type": out["type"],
        "lat": out["lat"],
        "lon": out["lon"],
        "fire": out["fire"],
        "is_cn": out["is_cn"],
        "label": out["label"],
        "decision": out["decision"],                 # <— NEW
        "decision_detail": out["decision_detail"],   # <— NEW
        "pool_group": out["pool_group"],             # <— NEW  ('norcal'|'socal'|'mixed')
    })

result_cols = df.apply(apply_refine, axis=1)
df_refined = pd.concat([df, result_cols], axis=1)
# (a) % FALLBACK ALL HOTSPOTS
p_fallback_all = (df_refined["decision"] == "fallback_all_hotspots").mean()

# (b) % "no need to refine" (exact hotspot; includes CN refinement within same fire)
p_no_refine = (df_refined["decision"].isin(["exact_hotspot","exact_hotspot→refined_within_fire_for_cn"])).mean()

# (c) % by NorCal/SoCal of the chosen hotspot
group_props = df_refined["pool_group"].value_counts(normalize=True).round(4)

print("Proportion fallback_all_hotspots:", round(p_fallback_all, 4))
print("Proportion no_refine (exact hotspot):", round(p_no_refine, 4))
print("\nProportion by chosen pool_group:")
print(group_props)

# Optional: breakdown of all paths
print("\nDecision breakdown (all paths):")
print(df_refined["decision"].value_counts(normalize=True).round(4))

# Optional: cross-tab NorCal/SoCal within each decision path
ct = pd.crosstab(df_refined["decision"], df_refined["pool_group"], normalize="index").round(4)
print("\nDecision × pool_group (row-normalized):")
print(ct)

# --- save ---
df_refined.to_csv("saaaaaa_with_location_resolved.csv", index=False)



df_refined['place'].value_counts().plot(kind='bar')

import pandas as pd
import matplotlib.pyplot as plt

# Load your resolved file
df_refined = pd.read_csv("SA_with_location_resolved.csv")  # change if needed

# --- 1) Bar chart of all places ---
counts = df_refined['place'].value_counts()
colors = ['red' if place.lower() == 'paradise' else 'gray' for place in counts.index]

plt.figure(figsize=(10,5))
counts.plot(kind='bar', color=colors)
plt.title("Counts per Place (Paradise highlighted)")
plt.ylabel("Tweet count")
plt.tight_layout()
plt.show()

# --- 2) Scatter plot of jittered coords for Paradise ---
place_name = "Paradise"
subset = df_refined[df_refined["place"].str.lower() == place_name.lower()]

print(f"{len(subset)} tweets mentioning {place_name}")

plt.figure()
plt.scatter(subset["lon"], subset["lat"], s=20, alpha=0.7, color='red', label=place_name)
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title(f"Jittered coordinates for {place_name}")
plt.legend()
plt.tight_layout()
plt.show()